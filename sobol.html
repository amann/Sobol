<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-12-18 Mi 14:28 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org864f95a">1. Sobol' Numbers in Financial Monte-Carlo Simulations</a>
<ul>
<li><a href="#org9d08af0">1.1. Valuation of Instruments using an Economic Scenario Model</a></li>
<li><a href="#org2f36c06">1.2. Dependence Structure</a>
<ul>
<li><a href="#org157412c">1.2.1. Conditional Distribution Method</a></li>
<li><a href="#orgea961b1">1.2.2. Normal Distribution and Gaussian Copula</a></li>
<li><a href="#org61750c8">1.2.3. Cholesky Decomposition</a></li>
<li><a href="#orgeb298d2">1.2.4. Multivariate Standard Uniform Distribution</a></li>
</ul>
</li>
<li><a href="#org3179e9f">1.3. Monte Carlo and Quasi-Monte Carlo Methods</a>
<ul>
<li><a href="#org9e78510">1.3.1. The Koksma-Hlawka Inequality</a></li>
<li><a href="#org086d5b7">1.3.2. Definition Bounded Variation in the sense of Vitali-Lebesgue-Fréchet-de la Vallée Poussin</a></li>
<li><a href="#orgc6a9aad">1.3.3. Definition Bounded Variation in the sense of Hardy and Krause</a></li>
<li><a href="#orgd485258">1.3.4. Applying the Koksma-Hlawka Inequality</a></li>
</ul>
</li>
<li><a href="#org6977760">1.4. Sobol' Numbers</a>
<ul>
<li><a href="#org6c2a382">1.4.1. Generation of Sobol' Numbers</a></li>
<li><a href="#org7dcb18b">1.4.2. Correlation of the Coordinate Samples</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgac6094f">2. References</a></li>
</ul>
</div>
</div>
<p>
\(\newcommand{\corr}{\operatorname{\rho}}\)
\(\newcommand{\cov}{\operatorname{cov}}\)
\(\newcommand{\expect}[1]{\operatorname{\Bbb{E}}\left[#1\right]}\)
\(\newcommand{\proba}[1]{\operatorname{\Bbb{P}}\left[#1\right]}\)
\(\newcommand{\HKVar}{\operatorname{\mathrm{Var}_{\mathrm{HK}}}}\)
\(\newcommand{\abs}[1]{\left|#1\right|}\)
\(\newcommand{\set}[1]{\left\{#1\right\}}\)
\(\newcommand{\parens}[1]{\left(#1\right)}\)
\(\newcommand{\transpose}[1]{{#1}^{\mathrm{t}}}\)
\(\newcommand{\R}{\Bbb{R}}\)
\(\newcommand{\Rho}{\mathrm{P}}\)
\(\newcommand{\sampleMean}[3]{\frac{1}{#2}\sum_{#1=1}^{#2}{#3}}\)
\(\newcommand{\diag}[3]{\operatorname{diag}\parens{#1}_{#2=1}^{#3}}\)
\(\newcommand{\infiniteNorm}[1]{\left\lVert{}#1\right\rVert_\infty}\)
\(\newcommand{\scalarProd}[2]{\left\langle #1,#2\right\rangle}\)
\(\newcommand{\vector}[1]{\mathbfit{\boldsymbol{#1}}}\)
\(\newcommand{\component}[2]{\vector{#1}^{(#2)}}\)
\(\newcommand{\explicitVector}[2]{\transpose{\parens{#1, ..., #2}}}\)
\(\newcommand{\explicitComponentVector}[2]{\explicitVector{\component{#1}{1}}{\component{#1}{#2}}}\)
</p>




<div id="outline-container-org864f95a" class="outline-2">
<h2 id="org864f95a"><span class="section-number-2">1.</span> Sobol' Numbers in Financial Monte-Carlo Simulations</h2>
<div class="outline-text-2" id="text-1">
<p>
In this note, we discuss the possible use of Sobol' numbers in the generation process of economic scenarios for valuing balance sheets of complex financial instruments.
</p>
</div>

<div id="outline-container-org9d08af0" class="outline-3">
<h3 id="org9d08af0"><span class="section-number-3">1.1.</span> Valuation of Instruments using an Economic Scenario Model</h3>
<div class="outline-text-3" id="text-1-1">
<p>
We consider a risk free economic scenario model \(\mathcal{M}\) consisting of a set \(\mathcal{M} = \set{\varphi_{1,t_1}, \ldots, \varphi_{1,t_L}, \ldots, \varphi_{K,t_1}, \ldots, \varphi_{K,t_L}}\) of  \(K\times L\) random variables simulating the values of a price factor \(\varphi_k(t_l)\)  at time horizon \(t_l\).  These random variables may have functional dependences between each other, however, at the end, the whole set of scenarios is driven by a random vector \(\vector{X}=\explicitComponentVector{X}{D}:\Omega\to\R^D\) of dimension \(D\) whith multivariate distribution function \(H\) on a probability space \((\Omega,\mathcal{F},\Bbb{P})\) and whose components \(\component{X}{d} \sim F_d\) are random variables with some marginal distribution \(F_d(x) = \proba{\component{X}{d}\leq x}\).
</p>

<p>
The random value of a balance sheet of financial instruments in this model is a measurable function \(V_0:\R^D\to\R\) and the present value is \(\mathcal{V}=\expect{V_0(\vector{X})}\).
</p>

<p>
An important part of the model is the dependence structure of the random vector \(\vector{X}\) which we shall investigate in the next section.
</p>
</div>
</div>

<div id="outline-container-org2f36c06" class="outline-3">
<h3 id="org2f36c06"><span class="section-number-3">1.2.</span> Dependence Structure</h3>
<div class="outline-text-3" id="text-1-2">
<p>
By Sklar's theorem, there exists a unique copula \(C_H:[0,1]^D\to[0,1]\), with  \[\vector{H}(\vector{x}) = \explicitVector{F_1(\component{x}{1})}{F_D(\component{x}{D})}\text.\]
This means that the dependence structure of \(\vector{X}\) is expressed by the copula function which allows us to write the present value \(\mathcal{V}\) as
\[V = \expect{V(\vector{U})} = \expect{V_0(\vector{X})}\text,\]
where \(\vector{U}=\explicitComponentVector{U}{D}:\Omega\to\R^D\) is a random vector with distribution function \(C_H\) and the function \(V\) is defined as
\[V=V_0\circ\parens{F_1^-\times\ldots\times{}F_D^-}\]
with, for \(1\leq d\leq D\), the marginal quantile functions
\[F_d^-(p) = \inf\set{x\in\R\mid F_d(x)\geq p}\text.\]
</p>

<p>
Hence, if \(C_H\) and the margins \(F_d\), \(1\leq d\leq D\), are known, we can use Monte Carlo simulation to estimate \(\mathcal{V} = \expect{V(\vector{U})}\).  For a random sample \(\set{\vector{u}_1,...,\vector{u}_N}\) from \(C_H\), the Monte Carlo estimator of \(\mathcal{V}\) is
\[\sampleMean{i}{N}{V(\vector{u}_i)} \approx \expect{V(\vector{U})}\text.\]
</p>

<p>
The remaining problem is to find a way to sample \(\vector{U}\sim C_H\).
</p>
</div>

<div id="outline-container-org157412c" class="outline-4">
<h4 id="org157412c"><span class="section-number-4">1.2.1.</span> Conditional Distribution Method</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
A method for creating samples of \(\vector{U}\sim C_H\) is for instance described in <a href="#orgd390c68">[5]</a>, and is called <i>conditional distribution method (CDM)</i>.  The idea is, given a copula \(C\), to find a transformation function \(\phi_C:[0,1]^d\to[0,1]^d\) which transforms a uniformly distributed random vector \(\vector{U}' \sim \mathrm{U}[0,1,]^d\) into a random vector \(\vector{U} := \phi_C(\vector{U}') \sim C\).
</p>

<p>
In this section, we use following notation. If \(\vector{x} = \explicitComponentVector{x}{d}\) is a (random) vector of dimension \(d\), we write \(\vector{x}_{(k)} := \explicitComponentVector{x}{k}\) the restriction to the first \(k \leq d\) components of \(\vector{x}\).
</p>

<p>
For \(2\leq k \leq d\), let
\[C_{\vector{u}}(u) = \proba{\component{U}{k} \leq u \mid \vector{U}_{(k-1)} = \vector{u}}\]
denote the conditional copula of \(\component{U}{k}\) at  \(u\) given \(\vector{U}_{(k-1)}=\vector{u}\in[0,1]^{k-1}\)
and
\[C_{\vector{u}}^-(p) = \inf\set{u\mid C_{\vector{u}}(u) \geq p}\]
its quantile function. 
</p>

<p>
<b>Theorem</b>
</p>
<blockquote>
<p>
The recursively defined transformation function
</p>
\begin{align*}
 \phi_C : [0,1]^d &\to [0,1]^d\\
\vector{v} = \explicitComponentVector{v}{d} &\mapsto \vector{u}
 = \explicitVector{\component{v}{1}, C_{\vector{u}_{(1)}}^-(\component{v}{2})}
{C_{\vector{u}_{(d-1)}}^-(\component{v}{d})}
\end{align*}
<p>
transforms a uniformly distributed random vector \(\vector{U}' \sim \mathcal{U}[0,1]^d\) into a random vector \(\vector{U} := \phi_C(\vector{U}') \sim C\).
</p>
</blockquote>
<p>
As corollary, using Sklar's theorem, if \(H\) is a \(d\)-dimensional absolutely continuous distribution function with margins \(F_1, ..., F_d\) and copula \(C\), for \(1\leq k \leq d-1\) and \(\vector{u}\in[0,1]^k\), the conditional copula \(C_{\vector{u}}\) satisfies
\[C_{\vector{u}}(u) = H_{F_1^-\times...\times F_k^-(\vector{u})}\left(F_{k+1}^-(u)\right)\text,\]
where
\[H_{\vector{x}}(x)=\proba{\component{X}{k+1} \leq x \mid \vector{X}_{(k)} = \vector{x}}\text,\]
for some random vector \(\vector{X} \sim H\) and some \(k\)-dimensional vector \(\vector{x}\) and where
\[F_i^-(p) = \inf\set{x\mid F_i(x)\geq p}\]
is the quantile function of \(F_i\).
</p>

<p>
The economic models \(\mathcal{M}(\vector{X})\) we mostly use, depend on a \(D\)-dimensional multivariate normally distributed random vector \(\vector{X}\).  in the next section we shall investigate this case.
</p>
</div>
</div>

<div id="outline-container-orgea961b1" class="outline-4">
<h4 id="orgea961b1"><span class="section-number-4">1.2.2.</span> Normal Distribution and Gaussian Copula</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
We consider \(H = \Phi_{\vector{\mu},\Rho}\) the \(d\)-dimensional multivariate normal distribution function for which we suppose, without loss of generality, that the location vector \(\vector{\mu} = \vector{0}\) and the scale matrix \(\Rho\) is a correlation matrix of maximal rank, hence positive definite.
</p>

<p>
It can be shown (see <a href="#org5bc957d">[7]</a>), that for \(1 \leq k \leq d-1\) and a \(k\)-dimensional vector \(\vector{x}\) the conditional distribution
\(H_{\vector{x}}\) is normal.  More precisely
\[H_{\vector{x}}(x) = \Phi\left(\frac{x - \mu_k(\vector{x})} {\sqrt{\tilde{\rho}_k}}\right)\]
with
</p>
\begin{align*}
\mu_k(\vector{x}) &= \transpose{\vector{\rho}}_k \Rho_{(k)}^{-1} \vector{x}\text,\\
\tilde{\rho}_k &= \rho_k - \transpose{\vector{\rho}}_k \Rho_{(k)}^{-1} \vector{\rho}_k\\
\end{align*}
<p>
and where we decompose the correlation matrix \(\Rho_{(k+1)}\) into following blocks
\[\Rho = \left(\begin{matrix}\Rho_{(k)}&\vector{\rho}_k\\ \transpose{\vector{\rho}}_k& \rho_k \end{matrix}\right)\text,\]
with \(\Rho_{(k)}\) the upper left \(k\times k\) submatrix of \(\Rho_{(k+1)}\) and writing \(\Rho_{(d)}=\Rho\).
</p>

<p>
Hence, using the corollary above, the \(k\)-th conditional copula of the Gaussian copula \(C = \Phi_{\vector{0},\Rho}\circ\vector{\Phi^{-1}}\) is
\[C_{\vector{u}} = H_{\vector{\Phi}^{-1}(\vector{u})}\bigl(\Phi^{-1}(u)\bigr)
= \Phi\left(\frac{\Phi^{-1}(u) - \mu_k\bigl(\vector{\Phi}^{-1}(\vector{u})\bigr)}{\sqrt{\tilde{\rho}_k}}\right)\]
and its quantile function is
\[ C_{\vector{u}}^-(u) = \Phi\Bigl(\mu_k\bigl(\vector{\Phi}^{-1}(\vector{u})\bigr) + \sqrt{\tilde{\rho}_k}\cdot\Phi^{-1}(u)\Bigr)\]
and the transformation function \(\phi_C\) is
</p>
\begin{align*}
\vector{u} = \phi_C(\vector{v}) &= \explicitVector{\component{v}{1}, C_{\vector{u}_{(1)}}^-(\component{v}{2})} {C_{\vector{u}_{(d-1)}}^-(\component{v}{d})}\\
 &= \biggl(\component{v}{1}, \Phi\Bigl(\mu_1\bigl(\vector{\Phi}^{-1}(\vector{u}_{(1)})\bigr) + \sqrt{\tilde{\rho}_1}\cdot\Phi^{-1}(u)\Bigr), \ldots\\
 &\hphantom{ = \biggl(} \ldots,\Phi\Bigl(\mu_{d-1}\bigl(\vector{\Phi}^{-1}(\vector{u}_{(1)})\bigr) + \sqrt{\tilde{\rho}_{d-1}}\cdot\Phi^{-1}(u)\Bigr)\biggr)^{\mathrm{t}}
\end{align*}


<p>
For most copula families, the conditional distribution method is not optimal. (See <a href="#orgd390c68">[5]</a>.)  In particular for Gaussian copulas, a sampling can be obtained via their stochastic representation, as we shall sketch in the next section.
</p>
</div>
</div>

<div id="outline-container-org61750c8" class="outline-4">
<h4 id="org61750c8"><span class="section-number-4">1.2.3.</span> Cholesky Decomposition</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
The random vector \(\vector{X}\sim\Phi_{\vector{0},\Rho}\) admits the stochastic representation \(\vector{X} = A\vector{Z}\) where \(A\) denotes the lower triangular matrix from the Cholesky decomposition \(\Rho = A\transpose{A}\) and \(\vector{Z}\) is a vector of \(d\) independent standard normal random variables.  A random vector \(\vector{U}\) with the gaussian copula \(C = \Phi_{\vector{0},\Rho}\circ\vector{\Phi}^{-1}\), where  \(\vector{\Phi}^{-1}=\Phi^{-1}\times\ldots\times\Phi^{-1}\), as its distribution function, admits the stochastic representation \(\vector{\Phi}(X) = \vector{\Phi}(A\vector{Z}) = \vector{\Phi}\bigl(A\vector{\Phi}^{-1}(\vector{U}')\bigr)\) for the multivariate uniformly distributed random vector \(\vector{U}' \sim \mathcal{U}[0,1]^d\).
</p>

<p>
According to <a href="#orgd390c68">[5]</a>, this sampling approach is equivalent to the CDM described before.
</p>


<p>
We have now an expression of our model \(\mathcal{M}\) as a function \(\mathcal{M}(\vector{U}')\) of a multivariate standard uniformly distributed random vector \(\vector{U}' \sim \mathcal{U}[0,1]^d\) and our Monte Carlo estimator of the present value \(\mathcal{V}\) of the balance sheet is
\[\sampleMean{i}{N}{V(\vector{u}_i)} \approx \expect{V(\vector{U}')}\]
for a (pseudo/quasi) random sample \(\set{\vector{u}_1,\ldots,\vector{u}_N}\) from the multivariate standard uniform distribution \(\mathcal{U}[0,1]^D\).
</p>
</div>
</div>

<div id="outline-container-orgeb298d2" class="outline-4">
<h4 id="orgeb298d2"><span class="section-number-4">1.2.4.</span> Multivariate Standard Uniform Distribution</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
This section is just here for noting that the marginal distributions of a multivariate standard uniform distribution \(\mathcal{U}[0,1]^d\) are independent univariate standard uniform distributions on \([0,1]\).  Indeed for \(\vector{U}\sim\mathcal{U}[0,1]^d\) and for every measurable subset \(A\subseteq[0,1]^d\) we have \[\proba{U\in A} = \int_Adu\text.\]  In particular, for \(A=\prod_{i=1}^d[a_i,b_i]\), with \(0\leq a_i\leq b_i\leq 1\), we have
</p>
\begin{align*}
\proba{U\in A} &= \int_Adu = \int_{\prod_{i=1}^d[a_i,b_i]}d\vector{u} = \int_{a_1}^{b_1}d\component{u}{1}\cdot\ldots\cdot\int_{a_d}^{b_d}d\component{u}{d}\\ &= \proba{\component{U}{1}\in[a_1,b_1]}\cdot\ldots \cdot\proba{\component{U}{d}\in[a_d,b_d]}\text.
\end{align*}
</div>
</div>
</div>

<div id="outline-container-org3179e9f" class="outline-3">
<h3 id="org3179e9f"><span class="section-number-3">1.3.</span> Monte Carlo and Quasi-Monte Carlo Methods</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Quasi-Monte Carlo methods are used for numerical integration by using low-discrepancy sequences.  The problem which the Monte Carlo as well as the quasi-Monte Carlo methods try to solve, is to approximate the integral of a function \(f\) on, say, the \(d\)-dimensional unit cube as the average of the function evaluated at a set of points \(\vector{u}_1, ..., \vector{u}_N \in [0,1]^d\):
\[\int_{[0,1]^d}f(u)du\approx\sampleMean{1}{N}{f(\vector{u}_i)}\text.\]
The difference between the Monte Carlo and the quasi-Monte Carlo methods is choice of the evaluation points \(\vector{u}_i\).  The Monte Carlo method uses pseudo-random numbers, whereas the quasi-Monte Carlo method uses low-discrepancy sequences.  The Koksma-Hlawka inequality gives us an upper bound for the approximation error by the product of two independent factors.  One of these factors depends only on the variation of the function \(f\) and the other factor depends only on a property (the star discrepancy) of the set of evaluation points \(\vector{u}_1, ..., \vector{u}_N\).  The discrepancy measures the deviation of the empirical distribution of those points and the Lebesgue measure \(\mu\).
</p>
</div>

<ol class="org-ol">
<li><a id="org29d81f2"></a>Definition Discrepancy<br />
<div class="outline-text-5" id="text-1-3-0-1">
<blockquote>
<p>
The <i>discrepancy</i> \(D_N(P)\) of a set \(P=\{\vector{u}_1, ..., \vector{u}_N\}\subset\prod_{i=1}^d[0,1) \subset\Bbb R^d\) is
\[D_N(P) = \sup_{B \in J}\left|\frac{\sharp(B\cap P)}N - \mu(B)\right|\text,\]
where \(\mu\) is the \(d\)-dimensional Lebesgue mesure,  and \(J\) is the set of  \(d\)-dimensional intervals \(\prod_{i=1}^d[a_i,b_i)\) with \(0\leq a_i\leq b_i < 1\).
</p>
</blockquote>

<p>
A slightly simpler characterization of discrepancy is the star discrepancy.
</p>
</div>
</li>

<li><a id="org26c0df2"></a>Definition Star Discrepancy<br />
<div class="outline-text-5" id="text-1-3-0-2">
<blockquote>
<p>
The <i>star discrepancy</i> \(D_N^*(P)\) of a set \(P=\{\vector{u}_1, ..., \vector{u}_N\}\subset\prod_{i=1}^d[0,1) \subset\Bbb R^d\) is
\[D_N^*(P) = \sup_{B \in J^*}\left|\frac{\sharp(B\cap P)}N - \mu(B)\right|\text,\]
where \(\mu\) is the \(d\)-dimensional Lebesgue mesure,  and \(J^*\) is the set of  \(d\)-dimensional intervals \(\prod_{i=1}^d[0,u_i)\) with \(u_i\in[0,1)\).
</p>
</blockquote>

<p>
The two are related by
\[D_N^* \leq D_N \leq 2^dD_N^*\text.\]
</p>

<p>
In order to be able to quantify the convergence of the quasi-Monte Carlo method  for integration,  the integrand function \(f\) needs to satisfy some bounded variation condition.
</p>
</div>
</li>

<li><a id="org075bc1c"></a>Lower Bounds<br />
<div class="outline-text-5" id="text-1-3-0-3">
<p>
It is conjuctured that the lower bound of the star discrepancy for each finite set \(\set{\vector{u}_1, ...,\vector{u}_N}\subset[0,1]^d\) is \(C_d\frac{\log(N)^{d-1}}{N}\) for some constant \(C_D>0\) depending only on the dimension \(d\).
</p>
</div>
</li>

<li><a id="org5ba44da"></a>Estimation of the Discrepancy<br />
<div class="outline-text-5" id="text-1-3-0-4">
<p>
Directly computing the discrepancy of a sequence of numbers is exponentially difficult with growing dimension.  The <i>Erdős-Turán-Koksma</i> inequality provides an upper bound for the star discrepancy:  For points \(\vector{u}_1, ..., \vector{u}_N\) in \([0,1]^d\) and some arbitrary positive integer \(H\), we have
</p>
\begin{align*}
D_N^*(\vector{u}_1, ..., \vector{u}_N)
\leq \left(\frac{3}{2}\right)^d
\biggl(\frac{2}{H+1}
 +\hspace{-1em}\sum_{\vector{h}\in\Bbb{Z}^d \atop :0 < \infiniteNorm{\vector{h}}\leq H}
 \frac1{\prod_{i=1}^d\max\set{1,\abs{\component{h}{i}}}}
     \abs{ \sampleMean{n}{N}{e^{2\pi i\scalarProd{\vector{h}}{\vector{u}_n}}}}\biggr)\text.
\end{align*}
</div>
</li>
</ol>


<div id="outline-container-org9e78510" class="outline-4">
<h4 id="org9e78510"><span class="section-number-4">1.3.1.</span> The Koksma-Hlawka Inequality</h4>
<div class="outline-text-4" id="text-1-3-1">
<blockquote>
<p>
If \(f\) is a function which has <i>bounded variation</i> \(\HKVar(f)\) on the \(d\)-dimensional unit cube \([0,1]^d\) in the sense of <i>Hardy and Krause</i>, then for any set of points \(\vector{u}_1, ..., \vector{u}_N\) in \([0,1)^d\), we have
\[\abs{\frac{1}{N}\sum_{i=1}^Nf(\vector{u}_i) - \int_{[0,1)^d}f(\vector{u})\,du} \leq \HKVar(f)D_N^*(\vector{u}_1,...,\vector{u}_N)\text.\]
</p>
</blockquote>

<p>
Let \(\vector{a},\vector{b}\in[0,1]^d\) for which the coefficients satisfy \(\component{a}{i} < \component{b}{i}\) for all \(i=1, ...,d\). We write in this case \[\vector{a} < \vector{b}\] and the hyper-rectangle \[[\vector{a},\vector{b}] := \prod_{i=1}^d[\component{a}{i},\component{b}{i}]\text.\]
This hyper rectangle has \(2^d\) corners which are exactly those points \(\vector{u}\) whose coefficients are either coefficients of \(\vector{a}\) or coefficients of \(\vector{b}\). For a corner \(\vector{u}\) of \([\vector{}a,\vector{b}]\), we denote by \(\sharp_{\vector{a}}^{\vector{u}}\) the number of coefficients of \(\vector{u}\) which are coefficients of \(\vector{a}\).
</p>

<p>
Consider now a function \(f\) defined on the \(d\)-dimensional unit cube \([0,1]^d\). 
</p>

<p>
The \(d\)-fold alternate sum of \(f\) over the hyper rectangle \([\vector{a},\vector{b}]\)  is
\[\Delta(f;\vector{a},\vector{b}) = \sum_{\vector{u}\in\mathrm{Corners\ of }[\vector{a},\vector{b}]}(-1)^{\sharp_{\vector{a}}^{\vector{u}}}f(\vector{u})\]
</p>

<p>
For each dimension \(i\), we consider a sequence \(0 = \component{c_0}{i} <...< \component{c_{k_i}}{i} = 1\).  The cartesian product \(\mathcal{C} = \prod_{i=1}^d\set{\component{c_0}{i}, ..., \component{c_{k_i}}{i}} \subset [0,1]^d\) consists of the corners of a pavement of \([0,1]^d\) by hyper rectangles \([\vector{c},\vector{c}']\) with \(\vector{c},\vector{c}'\in\mathcal{C}\) and \(\vector{c} < \vector{c}'\).
</p>

<p>
The variation of \(f\) over \(\mathcal{C}\) is
\[V_{\mathcal{C}}(f)=\sum_{\vector{c},\vector{c}'\in\mathcal{C}:\atop \vector{c} < \vector{c}'}\abs{\Delta(f;\vector{c},\vector{c}')}\text.\]
</p>
</div>
</div>

<div id="outline-container-org086d5b7" class="outline-4">
<h4 id="org086d5b7"><span class="section-number-4">1.3.2.</span> Definition Bounded Variation in the sense of Vitali-Lebesgue-Fréchet-de la Vallée Poussin</h4>
<div class="outline-text-4" id="text-1-3-2">
<blockquote>
<p>
The <i>variation of \(f\) in the sense of Vitali</i> is
\[V_{d}(f) = \sup_{\mathcal{C}}V_{\mathcal{C}}(f)\text.\]
</p>
</blockquote>

<p>
For a subset \(I\subseteq\set{1, ..., d}\) we write
\[\left.[0,1]^d\right|_I = \set{\explicitComponentVector{u}{d}\in [0,1]^d\mid \component{u}{i} = 1 \text{ if } i\notin{I}}\] a \(\abs{I}\)-dimensional "face" or "edge" containing the corner \(\vector{1} := \explicitVector{1}{1}\) opposite to the corner \(\vector{0}\). Let \(\iota_I\) be the "canonical" injection  \(\iota_I:[0,1]^{\abs{I}}\hookrightarrow\left.[0,1]^d\right|_I\).  Write \(f_I = f \circ\iota_I\).
</p>
</div>
</div>

<div id="outline-container-orgc6a9aad" class="outline-4">
<h4 id="orgc6a9aad"><span class="section-number-4">1.3.3.</span> Definition Bounded Variation in the sense of Hardy and Krause</h4>
<div class="outline-text-4" id="text-1-3-3">
<blockquote>
<p>
The variation of \(f\) in the sense of Hardy and Krause is
\[\HKVar{f} = \sup_{I\subseteq\set{1, ..., d}} V_{\abs{I}}(f_I)\text.\]
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgd485258" class="outline-4">
<h4 id="orgd485258"><span class="section-number-4">1.3.4.</span> Applying the Koksma-Hlawka Inequality</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
If the valuation function \(V\) of the present value \(\mathcal{V}\) is bounded in the sense of Hardy and Krause, then, given a sample \(\set{u_1, ..., u_N}\) of the \(d\)-multivariate standard uniformly distributed random variable \(U\), we have
\[\abs{\sampleMean{i}{N}{V(\vector{u}_i)} - \expect{V(\vector{U})}} \leq \HKVar(V)\cdot D_N^*(\vector{u}_1, ..., \vector{u}_N)\text.\]
</p>
</div>
</div>
</div>

<div id="outline-container-org6977760" class="outline-3">
<h3 id="org6977760"><span class="section-number-3">1.4.</span> Sobol' Numbers</h3>
<div class="outline-text-3" id="text-1-4">
<p>
In order to improve the approximation result in Monte Carlo simulations, an idea is to use quasi-random numbers which generate low discrepancy sequences.  There are several methods for generating low discrepancy sequences. One of them are the Sobol' Numbers.
</p>

<p>
It can be shown that the star discrepancy of a \(d\)-dimensional Sobol' number sequence \(S=\set{s_1, ..., s_N}\) is bounded by
\(C\frac{\log(N)^d}{N}\) for some constant \(C>0\).  The conjectured lower bound for the star would mean that those sequences are in some sense optimal.  Notice however, that this upper bound begins its asymptotical behavior only for \(N > e^d\) which is a large number if \(d\) itself is large.  Therefore there is apriori no guarantee that in large dimension the convergence with Sobol' is better than with random number.
</p>
</div>

<div id="outline-container-org6c2a382" class="outline-4">
<h4 id="org6c2a382"><span class="section-number-4">1.4.1.</span> Generation of Sobol' Numbers</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
For generating Sobol' number sequences, see for instance <a href="#org23b3ab6">[8]</a>. The R library <code>SobolSequence</code> implements the algorithm by S. Joe and F. Y. Kuo (see <a href="https://cran.r-project.org/web/packages/SobolSequence/vignettes/sobolsequence.html">https://cran.r-project.org/web/packages/SobolSequence/vignettes/sobolsequence.html</a> and   <a href="https://web.maths.unsw.edu.au/~fkuo/sobol/">https://web.maths.unsw.edu.au/~fkuo/sobol/</a>) and also allows for generating randomized point by digital shift.
</p>
</div>
</div>

<div id="outline-container-org7dcb18b" class="outline-4">
<h4 id="org7dcb18b"><span class="section-number-4">1.4.2.</span> Correlation of the Coordinate Samples</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
A problem with the Sobol' sequence \(S=\set{\vector{s}_1, ..., \vector{s}_N}\subset[0,1]^d\) is that when we consider the samples of the coefficients \(\parens{\component{S}{1}, ..., \component{S}{d}} = \parens{\parens{\component{s_1}{1}, ..., \component{s_N}{1}}, ..., \parens{\component{s_1}{d}, ..., \component{s_N}{d}}}\), those \(\component{S}{i}\) are not independent for high dimensionality (\(d \gg 90)\). This means that the results explained in previous sections are not applicable.  In particular,  \(\vector\Phi^{-1}(S)\) is not a sample of a multivariate normal distribution which implies as consequence that for some \(d\times d\) matrix \(A\), the vectors \(A\vector\Phi^{-1}(S)\) are also not a sample of a multivariate normal distribution.  In fact, even the marginals \(\component{A\vector\Phi^{-1}(S)}{i}\) are not samples of normal distributions.
</p>
</div>
</div>
</div>
</div>













<div id="outline-container-orgac6094f" class="outline-2">
<h2 id="orgac6094f"><span class="section-number-2">2.</span> References</h2>
<div class="outline-text-2" id="text-2">
<p>
<a id="org8f56bff">[1]</a> James A. Clarkson, Raymond Adams, <i>On Definitions of Bounded Variation for Functions of Two Variables</i>, American Mathematical Society, 1932, <a href="https://www.ams.org/journals/tran/1933-035-04/S0002-9947-1933-1501718-2/S0002-9947-1933-1501718-2.pdf">https://www.ams.org/journals/tran/1933-035-04/S0002-9947-1933-1501718-2/S0002-9947-1933-1501718-2.pdf</a>
</p>

<p>
<a id="org065da02">[2]</a> <i>Low-discrepancy sequence</i>, Wikipedia, <a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">https://en.wikipedia.org/wiki/Low-discrepancy_sequence</a>
</p>

<p>
<a id="org3dc230e">[3]</a>  Christoph Aistleitner, Florian Pausinger, Anne Marie Svane, Robert F. Tichy, <i>On functions of bounded variation</i>, Mathematical Proceedings of the Cambridge Philosophical Society -1(3), June13 2016, <a href="https://arxiv.org/pdf/1510.04522">https://arxiv.org/pdf/1510.04522</a>
</p>

<p>
<a id="orgbc69b87">[4]</a> Art B. Owen, <i>Multidimensional variation for quasi-Monte Carlo</i>, Contemporary multivariate
analysis and design of experiments, 49-74, Ser. Biostat. 2, World Sci. Publ., Hackensack,
NJ, 2005, <a href="https://artowen.su.domains/reports/ktfang.pdf">https://artowen.su.domains/reports/ktfang.pdf</a>
</p>

<p>
<a id="orgd390c68">[5]</a> Mathieu Cambou, Marius Hofert, Christiane Lemieux, <i>Quasi-random numbers for copula models</i>, 2015, <a href="https://arxiv.org/pdf/1508.03483">https://arxiv.org/pdf/1508.03483</a>
</p>

<p>
<a id="org35aa463">[6]</a> Sumin Wang, Chenxian Huang, Yongdao Zhou, Min-Qian Liu, <i>An Efficient Quasi-Random Sampling for Copulas</i>, March 11 2024, <a href="https://arxiv.org/pdf/2403.05281">https://arxiv.org/pdf/2403.05281</a> 
</p>

<p>
<a id="org5bc957d">[7]</a> K'ai-T'ai Fang, Samuel Kotz, Kai Wang Ng, <i>Symmetric Multivariate and Related
 Distributions,</i> 1990, Chapman &amp; Hall/CRC
</p>

<p>
<a id="org23b3ab6">[8]</a> <i>Sobol sequence</i>, Wikipedia,
<a href="https://en.wikipedia.org/wiki/Sobol_sequence">https://en.wikipedia.org/wiki/Sobol_sequence</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2024-12-18 Mi 14:28</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
